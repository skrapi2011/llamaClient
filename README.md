# llamaClient
A simple React client for chatting with llama models. Chatting after project build doesn't require internet conneciton, as everything is run locally.

Currently there is only support for llama3

# Setup:
Ollama:
1. Download ollama from https://ollama.com/
2. Download llama3 model: `ollama run llama3`
3. Run ollama API: `ollama serve`

React:
1. Download Node from https://nodejs.org/en
2. After installing Node, run command prompt and execude: `npm install npx` to install React library
3. In command prompt, go to `app` directory and launch `npm i` to install required node modules
4. After module installation, you can run `npm start` to start a project and start chatting
